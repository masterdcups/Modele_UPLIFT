# -*- coding: utf-8 -*-
"""Uplift_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZE-TzbjeFLAFzTgW9-3xo4RqbYsgWJpv

# 1.Initialisation
"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('punkt')
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import numpy as np
from bs4 import BeautifulSoup
import csv
import pandas as pd
import re
import matplotlib.pyplot as plt
import math
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

"""# 2.Parseurs"""

# -*- coding: utf-8 -*-
"""
Created on Thu Mar 11 18:50:01 2021

@author: Pierre RIGAL, Romain MEUNIER
"""


###################################################################################################

"""
Entrée  : None
Sortie  : le numéro de tous articles présent dans le fichier
          All Bids of Conference - Papers per sub.htm
"""
def recupererArticles():
    articles = []
    with open("./data/Recommandation_Data/All Bids of Conference - Papers per sub.htm", "r" ) as f:
        html_doc = f.read()
    
    soup = BeautifulSoup(html_doc, features="lxml")
    
    for tr in soup.find('thead').find_next_sibling().find_all('tr'):
        articles.append(tr.get_text(separator=" ").split(".  ", 1)[0])
    
    f.close()
    return articles

###################################################################################################

"""
Entrée  : None
Sortie  : La liste contenant le Prénom / Nom de tous les lecteur
Note    : Dans la liste des lecteurs, tous les lecteurs ont forcément bid
"""
def recupererLecteur():
    lecteurs = []
    with open("./data/Recommandation_Data/All Bids of Conference - Papers per reviewer.htm", "r" ) as f:
        html_doc = f.read()
        
    soup = BeautifulSoup(html_doc, features="lxml")
    
    for tr in soup.find('thead').find_next_sibling().find_all('tr'):
        if tr.has_attr("style"):
            lecteurs.append(tr.get_text(separator=" "))
                
    f.close()
    return lecteurs
    
###################################################################################################

"""
Entrée  : name : Nom Prénom d'un lecteur
Sortie  : Liste contenant tous les noms d'article présent sur sa page google
          scholar
"""
def recupererArticlesGS(name):
    articlesGS = []
    p = re.compile('\[.*\]')

    try:
      f = open("./data/Recommandation_Data/"+name+" - Google Scholar.htm", "r" )
      html_doc = f.read()
      
      soup = BeautifulSoup(html_doc, features="lxml")
      
      for div in soup.find_all('div'):
          if div.get("id") == "gs_res_ccl_mid":
              for doc in div.find_all('div'):
                  if doc.has_attr("data-cid"):
                      article = doc.find('div', {"class": "gs_ri"}).find('h3').get_text(separator=" ")
                      balise = p.findall(article)
                      if balise != []:
                        balise = balise[0].split(" ")
                      article = article.split(" ")
                      n_article = []
                      for a in article:
                        if a not in balise and a != '':
                          n_article.append(a)
                      articlesGS.append(" ".join(n_article))
      
      f.close()
      return articlesGS
    except FileNotFoundError:
      pass

"""
Entrée  : None
Sortie  : Récupère la liste des tous les lecteurs présent dans le fichier
          PC Members.htm
Note    : La différence avec la fonction recupererLecteurBid() est que la liste
          de lecteur ici contient également les lecteurs qui n'ont pas bid
"""
def recupererLecteurPC():
    all_lecteurs = []
    try:
      f = open("./data/Recommandation_Data/PC Members.htm", "r" )
      html_doc = f.read()
          
      soup = BeautifulSoup(html_doc, features="lxml")
      bid_lec = []
      
      for t in soup.find_all('table'):
        if t.get("id") == "ec:table2":
          for tr in t.find("tbody"):
            lec = tr.get_text(separator=" ").split(" ")[0:2]
            lec = " ".join(lec)
            all_lecteurs.append(lec)

      f.close()
      return all_lecteurs
    except FileNotFoundError:
      pass

"""
Entrée  : all_lecteurs : La liste contenant tous les lecteur
Sortie  : La liste de tous les articles (corpus)
"""
def get_all_article(all_lecteurs):
  all_article = [] 
  for al in all_lecteurs:
    articleGS = recupererArticlesGS(al)
    if articleGS == None:
      print(al)
    all_article.append(articleGS)     
  return all_article

"""
Entrée  : corpus : Une liste d'article
Sortie  : La liste des mots contenu dans le titre des articles
"""
def get_vocab(corpus):
  vectorizer = CountVectorizer()
  X = vectorizer.fit_transform(corpus)
  return vectorizer.get_feature_names()

"""
Entrée  : corpus : Une liste d'article
          vocabulary : Le vocabulaire associé à cette liste d'article
Sortie  : Récupère la valeur tfidf pour chaque mot de la liste de vocabulaire
"""
def get_tfidf(corpus, vocabulary):
  pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),
                 ('tfid', TfidfTransformer())]).fit(corpus)
  return list(pipe['tfid'].idf_)

"""
Entrée  : all_corpus : La liste d'article de tous les lecteurs
Sortie  : Le vocabulaire de tous les articles
Note    : Récupérer tous les mots sert pour la première ligne du csv
"""
def get_all_vocab(all_corpus):
  vocab = set([abs])
  for corpus in all_corpus:
    vocab = vocab | set(get_vocab(corpus))
  return list(vocab)

"""
Entrée  : all_lecteurs : La liste de tous les lecteurs
          lecteurs : La liste des tous les lecteurs qui ont bid
          all_article : La liste de tous les articles
Sortie  : None
Note    : Génère un csv : id lecteur | bid (0 ou 1) | liste des tous les mots 
                                                      contenu dans le titre des
                                                      articles
"""
def saveCSV(all_lecteurs, lecteurs, all_article):
    with open("bid.csv", "w", newline="") as f_write:
      writer = csv.writer(f_write)
      i = 0
      line = ["id", "bid"]
      all_vocab = get_all_vocab(all_article)
      writer.writerow(line + all_vocab)
      for al in all_lecteurs:
        line = [str(i)]
        if al in lecteurs:
          line.append("1")
        else:
          line.append("0")
        if all_article[i] != None:
          vocabulary = get_vocab(all_article[i])
          tfidf = get_tfidf(all_article[i], vocabulary)
          v_tfidf = dict(zip(vocabulary, tfidf))
          for av in all_vocab:
            if av in v_tfidf:
              line.append(str(v_tfidf.get(av)))
            else:
              line.append("0")
        i += 1
        writer.writerow(line)
      f_write.close()

"""
Entrée  : id_reviewer : un dictionnaire faisant le lien entre chaque reviewer
          et son id
          submission : une liste contenant le numéro de toutes les soumissions
          bid : liste des bids
Sortie  : une liste contenant des listes de la forme:
          [id soumission, id reviewer, 0 ou 1 si score confidence > 4]
"""
def get_score_confidence_v2(id_reviewer, submission, bid):
  try:
    f = open("./data/reviews/Reviews and Comments on Submission "+str(submission)+".htm", "r")
    all_score = []
    html_doc = f.read()
    soup = BeautifulSoup(html_doc, features="lxml")
    id_reviewer_in_submission = []

    try:
      scores = soup.find('table', {"id": "ec:table5"}).find('tbody')
    except:
      scores = soup.find('table', {"id": "ec:table4"}).find('tbody')

    if scores == [] or scores.find_parent('div').find_next_sibling() == None:
      scores = soup.find('table', {"id": "ec:table6"}).find('tbody')

    for s in scores:
      line = []
      if s.get("class")[0] == 'green':
        reviewer = s.find_all("td")[2].get_text(" ")
        line.append(str(id_reviewer.get(reviewer)))
        id_reviewer_in_submission.append(id_reviewer.get(reviewer))
        score_conf = int(s.get_text(" ").split(" ")[-2:-1][0])
        if score_conf > 3:
          line.append("1")
        else:
          line.append("0")
        all_score.append(line)

    for (id_r, id_a, b) in bid:
      if id_a == str(submission) and int(id_r) not in id_reviewer_in_submission:
        id_reviewer_in_submission.append(id_r)
        line = [str(id_r), str(b)]
        all_score.append(line)
    
    for (name, id) in id_reviewer.items():
      if id not in id_reviewer_in_submission:
        line = [str(id), str("0")]
        all_score.append(line)
    
    f.close()
    return all_score

  except FileNotFoundError:
    return None

"""
Entrée  : all_num_article : une liste contenant tous les id de soumission
          id_reviewer : un dictionnaire faisant le lien entre chaque reviewer
          et son id
          bid : liste des bids
Sortie  : None
Note    : Génère un csv : id article | id reviewer | 0 ou 1 si score confidence > 4
"""
def save_score_conf_csv_v2(all_num_article, id_reviewer, bid):
  for article in all_num_article:
    f = open("./score/score"+str(article)+".csv", "w", newline="")
    writer = csv.writer(f)
    line = ["id", "score"]
    writer.writerow(line)
    all_score = get_score_confidence_v2(id_reviewer, article, bid)
    if all_score != None:
      for line in all_score:
        writer.writerow(line)
    f.close()

"""
Entrée  : id_reviewer : un dictionnaire faisant le lien entre chaque reviewer
          et son id
Sortie  : liste des bids de chaque reviewer
Note    : liste des bids contient des tuple de la forme
          (id reviewer, id article, 1 si bid autre sinon)
"""
def get_bid(id_reviewer):
  bid = []
  id_lec_cour = None
  with open("./data/Recommandation_Data/All Bids of Conference - Papers per reviewer.htm", "r" ) as f:
      html_doc = f.read()
      
  soup = BeautifulSoup(html_doc, features="lxml")
  
  for tr in soup.find('thead').find_next_sibling().find_all('tr'):
      if tr.has_attr("style"):
          lecteur = tr.get_text(separator=" ")
          id_lec_cour = id_reviewer.get(lecteur)
      else:
          art = tr.find_next().get_text(separator=" ")
          b = tr.find_all_next()[3].get_text(separator=" ")
          if b == "yes":
              bid.append((id_lec_cour, art, 1))
          elif b == "maybe":
              bid.append((id_lec_cour, art, 0))
          else:
              bid.append((id_lec_cour, art, 0))
              
  f.close()
  return bid

"""
Entrée  : submission : une liste contenant le numéro de toutes les soumissions
Sortie  : création d'un txt pour chaque article contenant ses mots clés
"""
def get_key_word(submission):
  for s in submission:
    try:
      with open("./data/Recommandation_Data/Submission "+str(s)+".htm", "r" ) as f:
        html_doc = f.read()
        soup = BeautifulSoup(html_doc, features="lxml")

        key_word = soup.find('tr', {"id": "row12"}).find('td', {"class": "value"}).find_all("div")
        
        try:
          with open("./key_word/key_word"+str(s)+".txt", "w") as kwtxt:
            for kw in key_word:
              kwtxt.write(kw.get_text()+"\n")
        except:
          pass

    except:
      pass





#all_lecteurs : Liste de tous les lecteurs
all_lecteurs = recupererLecteurPC()

#lecteurs : Liste des lecteurs qui ont bid
lecteurs = recupererLecteur()

#all_lecteurs : union de all_lecteurs et de lecteurs pour ne pas oublier un lecteur
all_lecteurs = set().union(set(all_lecteurs),set(lecteurs))

#all_article : liste de tous les articles par lecteurs
all_article = get_all_article(all_lecteurs)

#Sauvegarde en csv le tfidf des mots par article dans un fichier bid.csv
saveCSV(all_lecteurs, lecteurs, all_article)

#Dictionnaire faisant le lien entre chaque reviewer et son id
id_reviewer = dict(zip(all_lecteurs, list(range(len(all_lecteurs)))))

#Tous les numéro de soumission
all_num_article = recupererArticles()

#Liste de tuple contenant (id reviewer, id article, 1 si bid autre sinon)
bid = get_bid(id_reviewer)


#Création d'un csv de score par article pour stocker le score de confiance
save_score_conf_csv_v2(all_num_article, id_reviewer, bid)


#Création d'un txt contenant les mots clés de chaque article
get_key_word(all_num_article)

"""#3.Récupération des données

"""

"""
Entrée  : None
Sortie  : Récupère les lecteurs dans le fichier crée bid.csv et convertit le fichier en matrice numpy
"""

def recupererLecteurFromBid():
  data=pd.read_csv('./bid.csv')
  data=data.to_numpy()
  return data

"""
Entrée  : un entier numarticle
Sortie  : Récupère les labels de chaque reviewers pour un article numarticle
"""
def recupererLabelforAnArticle(numarticle):
  label=pd.read_csv('./score/score'+str(numarticle)+'.csv')
  label=label.to_numpy()
  return label

"""
Entrée  : les données récupérees grace a recupererLecteurFromBid
Sortie  : les données séparées entre les données de traitement et les données de controle,
 ainsi qu'une liste des identifiants liés séparés de la même facon
"""
def split_data(data):
  data_control = []
  id_control =[]
  data_treatment = []
  id_treatment = []
  for i in data:
    if i[1]==0: 
      data_treatment.append(i)
      id_treatment.append(i[0])
    else :
      data_control.append(i)
      id_control.append(i[0])
  data_control=sorted(data_control, key = lambda data : data[0])
  data_control=np.array(data_control)
  data_control=np.delete(data_control,0,1)
  data_control=np.delete(data_control,0,1)
  data_treatment=sorted(data_treatment, key = lambda data : data[0])
  data_treatment=np.array(data_treatment)
  data_treatment=np.delete(data_treatment,0,1)
  data_treatment=np.delete(data_treatment,0,1)
  id_control=np.array(id_control)
  id_treatment=np.array(id_treatment)
  return data_control,data_treatment, id_control,id_treatment

data=recupererLecteurFromBid()
data_control,data_treatment,id_control,id_treatment=split_data(data)

"""
Entrée  : les labels d'un article avec les reviwers associé, 
          la liste des identifiants des reviwers du jeu de controle
          et la liste des identifients des reviewers du jeu de traitement
Sortie  : liste label_contol et label_traitement qui correnspondent au label respectivement
          liés au jeu de controle et au jeu de traitement
Note    : La fonction s'assure que chaque label soit bien associé au bon revierwer
"""
def association_label(label,id_control,id_treatment):
  label_control=[]
  label_treatment=[]
  for i in label :
    if np.any(id_control == i[0]) :
      label_control.append(i)
    else:
      label_treatment.append(i)
  label_control=sorted(label_control, key = lambda label : label[0])
  label_control=np.array(label_control)
  label_control=np.delete(label_control,0,1)
  label_treatment=sorted(label_treatment, key = lambda label : label[0])
  label_treatment=np.array(label_treatment)
  label_treatment=np.delete(label_treatment,0,1)
  return label_control,label_treatment

label=recupererLabelforAnArticle(14)
label_control,label_treatment=association_label(label,id_control,id_treatment)

#séparation des jeu de données en jeu de d'entrainement et jeu de test
trainC,testC,labeltrainC,labeltestC=train_test_split(data_control,label_control,test_size=0.2)
trainT,testT,labeltrainT,labeltestT=train_test_split(data_treatment,label_treatment,test_size=0.2)
test=np.concatenate((testC,testT))
labelTest=np.concatenate((labeltestC,labeltestT))

"""#4.Modèle uplift
##4.1 Random_Forest double classifieurs
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Feb 10 13:15:19 2021

@author: Pierre RIGAL, Romain MEUNIER
"""
#maj 27/03/21
#classe qui contient un modele uplift double classifieur avec random forest
class UpliftRF(object):
    #les randoms forest
    random_forest_control = RandomForestClassifier(n_estimators=2)
    random_forest_treatement = RandomForestClassifier(n_estimators=2)
    # n_estimators : nombre d'arbre
    # profondeur max des arbre
    #tous les jeux de données sont des numpy arrays
    def __init__(self, dA_control, dA_treatment , lA_control,lA_treatment,):
        self.dA_control = dA_control # DataSet d'apprentissage de l'arbe de controle
        self.dA_treatment = dA_treatment #Dataset d'apprentissage de l'arbre de traitement
        self.lA_control = lA_control # Labels d'apprentissage de l'arbre de controle
        self.lA_treatment = lA_treatment # Labels d'apprentissage de l'arbre de traitement

    #entrainement du modele uplift   
    def fit(self):
        self.random_forest_control.fit(self.dA_control, self.lA_control)
        self.random_forest_treatement.fit(self.dA_treatment, self.lA_treatment)
    
    #dT np array
    #prend un lecteur en entrée avec en paramètre les poids des mots des titre de ses articles
    #puis le modèle calcule si le lecteur peut être interresse ou non pour cet article
    def predict(self,dT):
        result_control = self.random_forest_control.predict(dT)
        result_treatment = self.random_forest_treatement.predict(dT)
        returnValue =[]
        for i in range(len(result_control)):
            if result_control[i]==1:
                returnValue.append(1)
            else :
                returnValue.append(np.max(result_treatment))
        return returnValue
    
    #calcul du score et comparaison avec modele sans uplift sur un graphe
    # X = jeu de données
    # Y = labels en tableau 1 dimension
    def Quini(self,X,y):
      Y_uplifted=[]
      Y_normal=[]
      ranged=[]
      for i in range(1,11):
        ranged.append(i/10)
        split_to_take= int(math.ceil(len(y)*(i/10)))
        Y_to_take=X[0:split_to_take]
        Y_normalise=y[0:split_to_take]
        uplifted=self.predict(Y_to_take)
        Y_uplifted.append(np.sum(uplifted))
        Y_normal.append(np.sum(Y_normalise))
      plt.plot(ranged, Y_uplifted,label="avec uplift")
      plt.plot(ranged, Y_normal,label="sans uplift")
      plt.legend()
      plt.show()

trainC,testC,labeltrainC,labeltestC=train_test_split(data_control,label_control,test_size=0.2)
trainT,testT,labeltrainT,labeltestT=train_test_split(data_treatment,label_treatment,test_size=0.2)
test=np.concatenate((testC,testT))
labelTest=np.concatenate((labeltestC,labeltestT))
U=UpliftRF(trainC,trainT,np.ravel(labeltrainC),np.ravel(labeltrainT))
U.fit()

U.Quini(test,np.ravel(labelTest))

"""Conclusion sur cette méthode : 

La méthode ne convient pas au problème posé. En effet, ici, le principal problème est que le jeu de donnée est trop petit. Ainsi, les forêts aléatoire n'ont aucun effet car chaque arbre de décision sera sensiblement simmilaire. De plus, le faible taux de cas positif entraine souvent une situation ou une majorité d'arbres ne sera entrainé qu'au cas négatif et donc ne pourront reconnaitre que des cas négatifs.
Dans cette situation, le nombre d'arbres optimal est 3 ce qui est insuffisant pour s'éloigner des performances d'un simple arbre de décision

Suite de la problématique :

Afin de résoudre ce problème de manque de données, nous allons essayer en premier lieu de passer sur des réseau de neurones qui

##4.2 Réseau de neurons double classifieurs
"""

#classe qui contient un modele uplift double classifieur avec reseau de neurone
class UpliftRN(object):
    
    
    # n_estimators : nombre d'arbre
    # profondeur max des arbre
    #tous les jeux de données sont des numpy arrays
    def __init__(self, dA_control, dA_treatment , lA_control,lA_treatment,):
        self.dA_control = dA_control # DataSet d'apprentissage de l'arbe de controle
        self.dA_treatment = dA_treatment #Dataset d'apprentissage de l'arbre de traitement
        self.lA_control = lA_control # Labels d'apprentissage de l'arbre de controle
        self.lA_treatment = lA_treatment # Labels d'apprentissage de l'arbre de traitement
        #reseaux de neurones
        self.model_control = keras.Sequential([
        keras.layers.Dense(64, activation='exponential',input_shape=(data_control.shape)),
        keras.layers.Dense(32, activation='sigmoid',input_shape=(data_control.shape)),
        keras.layers.Dense(1, activation='sigmoid'),])
        self.model_treatment = keras.Sequential([
        keras.layers.Dense(64, activation='exponential',input_shape=(data_control.shape)),
        keras.layers.Dense(32, activation='sigmoid',input_shape=(data_control.shape)),
        keras.layers.Dense(1, activation='sigmoid')
        ])

    def fit(self):
        self.model_control.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])
        self.model_treatment.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])
        self.model_control.fit(self.dA_control, self.lA_control,
          batch_size=1,
          epochs=8,
          verbose=1,
          validation_split=0.1,)
        self.model_treatment.fit(self.dA_treatment, self.lA_treatment,
          batch_size=1,
          epochs=8,
          verbose=1,
          validation_split=0.1,)
    
    #dT np array
    #prend un lecteur en entrée avec en paramètre les poids des mots des titre de ses articles
    #puis le modèle calcule si le lecteur peut être interresse ou non pour cet article
    def predict(self,dT):
        result_control = self.model_control.predict(dT)
        result_treatment = self.model_treatment.predict(dT)
        returnValue =[]
        for i in range(len(result_control)):
            if result_control[i]>=0.5:
                returnValue.append(1)
            else :
                if (np.max(result_treatment)) >= 0.5 :
                  returnValue.append(1)
                else:
                  returnValue.append(0)
        return returnValue
    
    def Quini(self,X,y):
      Y_uplifted=[]
      Y_normal=[]
      ranged=[]
      for i in range(1,11):
        ranged.append(i/10)
        split_to_take= int(math.ceil(len(y)*(i/10)))
        Y_to_take=X[0:split_to_take]
        Y_normalise=y[0:split_to_take]
        uplifted=self.predict(Y_to_take)
        Y_uplifted.append(np.sum(uplifted))
        Y_normal.append(np.sum(Y_normalise))
      plt.plot(ranged, Y_uplifted,label="avec uplift")
      plt.plot(ranged, Y_normal,label="sans uplift")
      plt.legend()
      plt.show()

trainC,testC,labeltrainC,labeltestC=train_test_split(data_control,label_control,test_size=0.2)
trainT,testT,labeltrainT,labeltestT=train_test_split(data_treatment,label_treatment,test_size=0.2)
test=np.concatenate((testC,testT))
labelTest=np.concatenate((labeltestC,labeltestT))
U=UpliftRN(trainC,trainT,labeltrainC,labeltrainT)
U.fit()

U.Quini(test,np.ravel(labelTest))

"""Conclusion : 

Le résultat des réseau de neurones sont plus encourageant car ceux-ci arrivent à déterminer des résultats positifs. Cependant cela permet de constater que nous nous heurtons aux problème moderne du modèle uplift, le calcul du score de précision est impossible sur des récoltes de données réelles.

En dernière solution, nous allons la méthode la plus évoluée du modèle uplift a été envisagée (le réseau de neurone monoclassifieur en comptant le bid non pas comme un séparateur en jeu de controle ou jeu de donnée mais comme une feature). Cependant cette méthode ne permet pas de l'adapter à notre problème et n'a donc pas été réalisée.

#5.Multi-cas
"""

#classe qui contient un systeme permettant de predire les personnes interessee pour un article a partir d'autres articles
class Multi_Uplift(object):
  listeUplift = [] #liste de forêt aléatoire, chacune est associé à un article
  listeMotClef = [] #la liste des mots clefs associés à chaque articles
  def __init__(self, listeUplift, listeMotClef ):
        self.listeUplift = listeUplift
        self.listeMotClef = listeMotClef

  #ajoute une forêt aléatoire et ces mots clefs associés
  def add(self,model, motclefs):
     self.listeUplift.append(model)
     self.listeMotClef.append(motclefs)

     
  #on calcule pour chaque modèle déjà enregistré la similarité avec les mots clefs de l'article actuel puis
  #en fonction, du score retourné on prend plus ou moins en compte le résultat de ce modèle
  def predict(self,dT,motclef):
    vectorizer = TfidfVectorizer(
        ngram_range=(1,1)
    )
    j=len(self.listeUplift)
    score=np.zeros(dT.shape[0])
    k=0
    for i in range(j):
      tfidf = vectorizer.fit_transform([self.listeMotClef[i],motclef])
      a= np.array(self.listeUplift[i].predict(dT))
      b=(((tfidf*tfidf.T).A)[0,1])
      if b > 0.05 :
        k=k+1
      c=a*b
      score = score + c
    score = score/k
    return score
  
  def Quini(self,X,y,motclef):
      Y_uplifted=[]
      Y_normal=[]
      ranged=[]
      for i in range(1,11):
        ranged.append(i/10)
        split_to_take= int(math.ceil(len(y)*(i/10)))
        Y_to_take=X[0:split_to_take]
        Y_normalise=y[0:split_to_take]
        uplifted=self.predict(Y_to_take,motclef)
        Y_uplifted.append(np.sum(uplifted))
        Y_normal.append(np.sum(Y_normalise))
      plt.plot(ranged, Y_uplifted,label="avec uplift")
      plt.plot(ranged, Y_normal,label="sans uplift")
      plt.legend()
      plt.show()

My_Uplift=Multi_Uplift([],[])
data=recupererLecteurFromBid()
data_control,data_treatment,id_control,id_treatment=split_data(data)
data=np.delete(data,0,1)
data=np.delete(data,0,1)
for i in range(1,len(all_num_article)):
  label=recupererLabelforAnArticle(all_num_article[i])
  if label.size != 0 :
    label_control,label_treatment=association_label(label,id_control,id_treatment)
    with open("./key_word/key_word"+str(all_num_article[i])+".txt", "r") as txt:
      key_words=txt.read()
      key_words=key_words.lower()
      U=UpliftRN(data_control,data_treatment,label_control,label_treatment)
      U.fit()
      My_Uplift.add(U,key_words)

label=recupererLabelforAnArticle(14)
label=np.delete(label,0,1)
with open("./key_word/key_word14.txt", "r") as txt:
      key_words=txt.read()
      key_words=key_words.lower()
My_Uplift.Quini(data,np.ravel(label),key_words)