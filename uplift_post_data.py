# -*- coding: utf-8 -*-
"""Uplift_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZE-TzbjeFLAFzTgW9-3xo4RqbYsgWJpv

# Initialisation
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('punkt')
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras


"""#Récupération des données

"""

"""
Entrée  : None
Sortie  : Récupère les lecteurs dans le fichier crée bid.csv et convertit le fichier en matrice numpy
"""

def recupererLecteurFromBid():
  data=pd.read_csv('./bid.csv')
  data=data.to_numpy()
  return data

"""
Entrée  : un entier numarticle
Sortie  : Récupère les labels de chaque reviewers pour un article numarticle
"""
def recupererLabelforAnArticle(numarticle):
  label=pd.read_csv('./score/score'+str(numarticle)+'.csv')
  label=label.to_numpy()
  return label

"""
Entrée  : les données récupérees grace a recupererLecteurFromBid
Sortie  : les données séparées entre les données de traitement et les données de controle,
 ainsi qu'une liste des identifiants liés séparés de la même facon
"""
def split_data(data):
  data_control = []
  id_control =[]
  data_treatment = []
  id_treatment = []
  for i in data:
    if i[1]==0: 
      data_treatment.append(i)
      id_treatment.append(i[0])
    else :
      data_control.append(i)
      id_control.append(i[0])
  data_control=sorted(data_control, key = lambda data : data[0])
  data_control=np.array(data_control)
  data_control=np.delete(data_control,0,1)
  data_control=np.delete(data_control,0,1)
  data_treatment=sorted(data_treatment, key = lambda data : data[0])
  data_treatment=np.array(data_treatment)
  data_treatment=np.delete(data_treatment,0,1)
  data_treatment=np.delete(data_treatment,0,1)
  id_control=np.array(id_control)
  id_treatment=np.array(id_treatment)
  return data_control,data_treatment, id_control,id_treatment

data=recupererLecteurFromBid()
data_control,data_treatment,id_control,id_treatment=split_data(data)

"""
Entrée  : les labels d'un article avec les reviwers associé, 
          la liste des identifiants des reviwers du jeu de controle
          et la liste des identifients des reviewers du jeu de traitement
Sortie  : liste label_contol et label_traitement qui correnspondent au label respectivement
          liés au jeu de controle et au jeu de traitement
Note    : La fonction s'assure que chaque label soit bien associé au bon revierwer
"""
def association_label(label,id_control,id_treatment):
  label_control=[]
  label_treatment=[]
  for i in label :
    if np.any(id_control == i[0]) :
      label_control.append(i)
    else:
      label_treatment.append(i)
  label_control=sorted(label_control, key = lambda label : label[0])
  label_control=np.array(label_control)
  label_control=np.delete(label_control,0,1)
  label_treatment=sorted(label_treatment, key = lambda label : label[0])
  label_treatment=np.array(label_treatment)
  label_treatment=np.delete(label_treatment,0,1)
  return label_control,label_treatment

label=recupererLabelforAnArticle(14)
label_control,label_treatment=association_label(label,id_control,id_treatment)

#séparation des jeu de données en jeu de d'entrainement et jeu de test
trainC,testC,labeltrainC,labeltestC=train_test_split(data_control,label_control,test_size=0.2)
trainT,testT,labeltrainT,labeltestT=train_test_split(data_treatment,label_treatment,test_size=0.2)
test=np.concatenate((testC,testT))
labelTest=np.concatenate((labeltestC,labeltestT))

"""#4.Modèle uplift
##4.1 Random_Forest double classifieurs
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Feb 10 13:15:19 2021

@author: Pierre RIGAL, Romain MEUNIER
"""
#maj 27/03/21
#classe qui contient un modele uplift double classifieur avec random forest
class UpliftRF(object):
    #les randoms forest
    random_forest_control = RandomForestClassifier(n_estimators=2)
    random_forest_treatement = RandomForestClassifier(n_estimators=2)
    # n_estimators : nombre d'arbre
    # profondeur max des arbre
    #tous les jeux de données sont des numpy arrays
    def __init__(self, dA_control, dA_treatment , lA_control,lA_treatment,):
        self.dA_control = dA_control # DataSet d'apprentissage de l'arbe de controle
        self.dA_treatment = dA_treatment #Dataset d'apprentissage de l'arbre de traitement
        self.lA_control = lA_control # Labels d'apprentissage de l'arbre de controle
        self.lA_treatment = lA_treatment # Labels d'apprentissage de l'arbre de traitement

    #entrainement du modele uplift   
    def fit(self):
        self.random_forest_control.fit(self.dA_control, self.lA_control)
        self.random_forest_treatement.fit(self.dA_treatment, self.lA_treatment)
    
    #dT np array
    #prend un lecteur en entrée avec en paramètre les poids des mots des titre de ses articles
    #puis le modèle calcule si le lecteur peut être interresse ou non pour cet article
    def predict(self,dT):
        result_control = self.random_forest_control.predict(dT)
        result_treatment = self.random_forest_treatement.predict(dT)
        returnValue =[]
        for i in range(len(result_control)):
            if result_control[i]==1:
                returnValue.append(1)
            else :
                returnValue.append(np.max(result_treatment))
        return returnValue
    
    #calcul du score et comparaison avec modele sans uplift sur un graphe
    # X = jeu de données
    # Y = labels en tableau 1 dimension
    def Quini(self,X,y):
      Y_uplifted=[]
      Y_normal=[]
      ranged=[]
      for i in range(1,11):
        ranged.append(i/10)
        split_to_take= int(math.ceil(len(y)*(i/10)))
        Y_to_take=X[0:split_to_take]
        Y_normalise=y[0:split_to_take]
        uplifted=self.predict(Y_to_take)
        Y_uplifted.append(np.sum(uplifted))
        Y_normal.append(np.sum(Y_normalise))
      plt.plot(ranged, Y_uplifted,label="avec uplift")
      plt.plot(ranged, Y_normal,label="sans uplift")
      plt.legend()
      plt.show()

trainC,testC,labeltrainC,labeltestC=train_test_split(data_control,label_control,test_size=0.2)
trainT,testT,labeltrainT,labeltestT=train_test_split(data_treatment,label_treatment,test_size=0.2)
test=np.concatenate((testC,testT))
labelTest=np.concatenate((labeltestC,labeltestT))
U=UpliftRF(trainC,trainT,np.ravel(labeltrainC),np.ravel(labeltrainT))
U.fit()

U.Quini(test,np.ravel(labelTest))

"""Conclusion sur cette méthode : 

La méthode ne convient pas au problème posé. En effet, ici, le principal problème est que le jeu de donnée est trop petit. Ainsi, les forêts aléatoire n'ont aucun effet car chaque arbre de décision sera sensiblement simmilaire. De plus, le faible taux de cas positif entraine souvent une situation ou une majorité d'arbres ne sera entrainé qu'au cas négatif et donc ne pourront reconnaitre que des cas négatifs.
Dans cette situation, le nombre d'arbres optimal est 3 ce qui est insuffisant pour s'éloigner des performances d'un simple arbre de décision

Suite de la problématique :

Afin de résoudre ce problème de manque de données, nous allons essayer en premier lieu de passer sur des réseau de neurones qui

##4.2 Réseau de neurons double classifieurs
"""

#classe qui contient un modele uplift double classifieur avec reseau de neurone
class UpliftRN(object):
    
    
    # n_estimators : nombre d'arbre
    # profondeur max des arbre
    #tous les jeux de données sont des numpy arrays
    def __init__(self, dA_control, dA_treatment , lA_control,lA_treatment,):
        self.dA_control = dA_control # DataSet d'apprentissage de l'arbe de controle
        self.dA_treatment = dA_treatment #Dataset d'apprentissage de l'arbre de traitement
        self.lA_control = lA_control # Labels d'apprentissage de l'arbre de controle
        self.lA_treatment = lA_treatment # Labels d'apprentissage de l'arbre de traitement
        #reseaux de neurones
        self.model_control = keras.Sequential([
        keras.layers.Dense(64, activation='exponential',input_shape=(data_control.shape)),
        keras.layers.Dense(32, activation='sigmoid',input_shape=(data_control.shape)),
        keras.layers.Dense(1, activation='sigmoid'),])
        self.model_treatment = keras.Sequential([
        keras.layers.Dense(64, activation='exponential',input_shape=(data_control.shape)),
        keras.layers.Dense(32, activation='sigmoid',input_shape=(data_control.shape)),
        keras.layers.Dense(1, activation='sigmoid')
        ])

    def fit(self):
        self.model_control.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])
        self.model_treatment.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])
        self.model_control.fit(self.dA_control, self.lA_control,
          batch_size=1,
          epochs=8,
          verbose=1,
          validation_split=0.1,)
        self.model_treatment.fit(self.dA_treatment, self.lA_treatment,
          batch_size=1,
          epochs=8,
          verbose=1,
          validation_split=0.1,)
    
    #dT np array
    #prend un lecteur en entrée avec en paramètre les poids des mots des titre de ses articles
    #puis le modèle calcule si le lecteur peut être interresse ou non pour cet article
    def predict(self,dT):
        result_control = self.model_control.predict(dT)
        result_treatment = self.model_treatment.predict(dT)
        returnValue =[]
        for i in range(len(result_control)):
            if result_control[i]>=0.5:
                returnValue.append(1)
            else :
                if (np.max(result_treatment)) >= 0.5 :
                  returnValue.append(1)
                else:
                  returnValue.append(0)
        return returnValue
    
    def Quini(self,X,y):
      Y_uplifted=[]
      Y_normal=[]
      ranged=[]
      for i in range(1,11):
        ranged.append(i/10)
        split_to_take= int(math.ceil(len(y)*(i/10)))
        Y_to_take=X[0:split_to_take]
        Y_normalise=y[0:split_to_take]
        uplifted=self.predict(Y_to_take)
        Y_uplifted.append(np.sum(uplifted))
        Y_normal.append(np.sum(Y_normalise))
      plt.plot(ranged, Y_uplifted,label="avec uplift")
      plt.plot(ranged, Y_normal,label="sans uplift")
      plt.legend()
      plt.show()

trainC,testC,labeltrainC,labeltestC=train_test_split(data_control,label_control,test_size=0.2)
trainT,testT,labeltrainT,labeltestT=train_test_split(data_treatment,label_treatment,test_size=0.2)
test=np.concatenate((testC,testT))
labelTest=np.concatenate((labeltestC,labeltestT))
U=UpliftRN(trainC,trainT,labeltrainC,labeltrainT)
U.fit()

U.Quini(test,np.ravel(labelTest))

"""Conclusion : 

Le résultat des réseau de neurones sont plus encourageant car ceux-ci arrivent à déterminer des résultats positifs. Cependant cela permet de constater que nous nous heurtons aux problème moderne du modèle uplift, le calcul du score de précision est impossible sur des récoltes de données réelles.

En dernière solution, nous allons la méthode la plus évoluée du modèle uplift a été envisagée (le réseau de neurone monoclassifieur en comptant le bid non pas comme un séparateur en jeu de controle ou jeu de donnée mais comme une feature). Cependant cette méthode ne permet pas de l'adapter à notre problème et n'a donc pas été réalisée.

#5.Multi-cas
"""

#classe qui contient un systeme permettant de predire les personnes interessee pour un article a partir d'autres articles
class Multi_Uplift(object):
  listeUplift = [] #liste de forêt aléatoire, chacune est associé à un article
  listeMotClef = [] #la liste des mots clefs associés à chaque articles
  def __init__(self, listeUplift, listeMotClef ):
        self.listeUplift = listeUplift
        self.listeMotClef = listeMotClef

  #ajoute une forêt aléatoire et ces mots clefs associés
  def add(self,model, motclefs):
     self.listeUplift.append(model)
     self.listeMotClef.append(motclefs)

     
  #on calcule pour chaque modèle déjà enregistré la similarité avec les mots clefs de l'article actuel puis
  #en fonction, du score retourné on prend plus ou moins en compte le résultat de ce modèle
  def predict(self,dT,motclef):
    vectorizer = TfidfVectorizer(
        ngram_range=(1,1)
    )
    j=len(self.listeUplift)
    score=np.zeros(dT.shape[0])
    k=0
    for i in range(j):
      tfidf = vectorizer.fit_transform([self.listeMotClef[i],motclef])
      a= np.array(self.listeUplift[i].predict(dT))
      b=(((tfidf*tfidf.T).A)[0,1])
      if b > 0.05 :
        k=k+1
      c=a*b
      score = score + c
    score = score/k
    return score
  
  def Quini(self,X,y,motclef):
      Y_uplifted=[]
      Y_normal=[]
      ranged=[]
      for i in range(1,11):
        ranged.append(i/10)
        split_to_take= int(math.ceil(len(y)*(i/10)))
        Y_to_take=X[0:split_to_take]
        Y_normalise=y[0:split_to_take]
        uplifted=self.predict(Y_to_take,motclef)
        Y_uplifted.append(np.sum(uplifted))
        Y_normal.append(np.sum(Y_normalise))
      plt.plot(ranged, Y_uplifted,label="avec uplift")
      plt.plot(ranged, Y_normal,label="sans uplift")
      plt.legend()
      plt.show()

My_Uplift=Multi_Uplift([],[])
data=recupererLecteurFromBid()
data_control,data_treatment,id_control,id_treatment=split_data(data)
data=np.delete(data,0,1)
data=np.delete(data,0,1)
all_num_article=['14', '15', '19', '22', '23', '24', '26', '27', '28', '29', '30', '31', '32', '33', '34', '36', '37', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '57', '59', '60', '66', '67']
for i in range(1,len(all_num_article)):
  label=recupererLabelforAnArticle(all_num_article[i])
  if label.size != 0 :
    label_control,label_treatment=association_label(label,id_control,id_treatment)
    with open("./key_word/key_word"+str(all_num_article[i])+".txt", "r") as txt:
      key_words=txt.read()
      key_words=key_words.lower()
      U=UpliftRN(data_control,data_treatment,label_control,label_treatment)
      U.fit()
      My_Uplift.add(U,key_words)

label=recupererLabelforAnArticle(14)
label=np.delete(label,0,1)
with open("./key_word/key_word14.txt", "r") as txt:
      key_words=txt.read()
      key_words=key_words.lower()
My_Uplift.Quini(data,np.ravel(label),key_words)